{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77544573-6731-4881-8ee1-9853d8da4f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introducción a PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d7a7a1c-7ae3-47b4-8d7d-1b901fc7d5ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "PySpark es la API de Python para Apache Spark, un motor de procesamiento distribuido diseñado para trabajar con grandes volúmenes de datos de forma eficiente.\n",
    "\n",
    "Spark trabaja principalmente con DataFrames, que conceptualmente son similares a los DataFrames de pandas, pero:\n",
    "- Están distribuidos\n",
    "- Son inmutables\n",
    "- Las operaciones son lazy (no se ejecutan hasta que se necesita el resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3d63408-9ce6-4574-bd0b-e7d9de51efd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preparacion entorno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f34b253b-4f13-4043-bc35-4fc3677af26f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Si no estuvieramos en Databricks habría que poner lo siguiente:\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Intro PySpark\") \\\n",
    "    .getOrCreate()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "050d2662-7d96-42ac-8d9b-2b05905dab58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pero nos lo podemos saltar y comenzar con esto:\n",
    "\n",
    "Copia lo siguiente en una celda. Vamos a crear un catalogo, schema y volumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54c8a5ed-9ada-4f13-bd30-15128a157d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```sql\n",
    "create catalog if not exists sesion1;\n",
    "create schema if not exists sesion1.sparkintro;\n",
    "create volume if not exists sesion1.sparkintro.landing;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bac594c5-8735-4f88-a023-942b290fe7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Trameos del Github la información al volumen\n",
    "\n",
    "```sql\n",
    "%sh\n",
    "curl -L https://raw.githubusercontent.com/jmartinezceste/Databricks_master/refs/heads/main/sesion1/data/spark_intro.csv -o /Volumes/sesion1/sparkintro/landing/spark_intro.csv\n",
    "\n",
    "%sh\n",
    "curl -L https://raw.githubusercontent.com/jmartinezceste/Databricks_master/refs/heads/main/sesion1/data/dim_spark_intro.csv -o /Volumes/sesion1/sparkintro/landing/dim_spark_intro.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39da539c-1e57-4e86-ac59-2185797b5321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creamos el DF\n",
    "Ejecuta el siguiente comando en el notebook de ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "455c25b7-6ac8-4dfd-850c-861edc35fba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read CSV from volume as a Spark DataFrame\n",
    "sales_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)  # Let Spark infer column types for now\n",
    "    .csv(\"/Volumes/sesion1/sparkintro/landing/spark_intro.csv\")\n",
    ")\n",
    "\n",
    "display(sales_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4e53382-110b-4b2e-bc82-801d4b469a70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Es importante entender el schema de los datos. Para ello ejecutamos\n",
    "```python\n",
    "sales_df.printSchema()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f357a094-0e9d-4970-827d-da931d290f7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Select(): elegir columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6790b316-1363-4f3a-9735-016abb72805e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "select() sirve para proyectar columnas, es decir:\n",
    "- elegir solo aquellas columnas que nos interesan\n",
    "- crear nuevas columnas calculadas (junto con withColumn)\n",
    "\n",
    "Ideas importantes:\n",
    "- Es una narrow transformation: cada partición puede ejecutar el select sin necesidad de hablar con otras particiones.\n",
    "- Es barata y muy común en pipelines reales.\n",
    "\n",
    "Se parece mucho al SELECT columna1, columna2 FROM tabla de SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1f7d5d3-b996-493f-a2ab-ee2f05f9451e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "# Select a subset of columns\n",
    "sales_simple_df = sales_df.select(\"order_id\", \"order_date\", \"country\", \"units_sold\")\n",
    "display(sales_simple_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7f69daa-4cfd-4fa1-9a54-7aaa7b97b33a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## filter() / where(): filtrar filas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a5a924a-c636-4618-87ea-4225c2cd9b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`filter()` (o su alias `where()`) nos permite quedarnos solo con las filas que cumplen una condición lógica.\n",
    "\n",
    "Ejemplos:\n",
    "- ventas solo de un país\n",
    "- pedidos con más de X unidades\n",
    "\n",
    "Características:\n",
    "- También es una narrow transformation.\n",
    "- Cada partición decide, con sus propias filas, qué se queda y qué se descarta.\n",
    "- Spark puede aplicar técnicas como predicate pushdown para optimizar la lectura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0a231d5-3958-4b99-8a0f-4d067e5514a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "# Filter sales only for Spain\n",
    "spain_sales_df = sales_df.filter(F.col(\"country\") == \"Spain\")\n",
    "display(spain_sales_df)\n",
    "\n",
    "# Filter sales with more than 3 units sold\n",
    "big_orders_df = sales_df.filter(F.col(\"units_sold\") > 3)\n",
    "display(big_orders_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "205f6ba3-a556-408f-8005-a6a395a54b15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## withColumn(): crear o transformar columnas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a79636e-74e1-47fb-9ebe-9842039719d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`withColumn()` se usa para:\n",
    "- crear columnas nuevas a partir de otras\n",
    "- transformar valores de columnas existentes\n",
    "\n",
    "En Spark los DataFrames son inmutables:\n",
    "- no modificamos el DataFrame original\n",
    "- siempre devolvemos un DataFrame nuevo con los cambios\n",
    "\n",
    "Esto permite:\n",
    "- razonar mejor sobre los pipelines\n",
    "- que Spark reordene y optimice las operaciones internamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7002f3a8-4f76-4b0e-b88f-2a5ce0c7c0eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "from pyspark.sql import functions as\n",
    "\n",
    "# Create a new column with total sales amount\n",
    "sales_with_total_df = sales_df.withColumn(\n",
    "    \"total_sales\",\n",
    "    F.col(\"units_sold\") * F.col(\"unit_price\")  # Simple numeric expression\n",
    ")\n",
    "\n",
    "display(sales_with_total_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75692e97-213b-4570-bf05-65f185afcfe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## groupBy().agg(): agregaciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e16ce14d-fc2a-4d40-a221-b1e272e504e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`groupBy().agg()` nos permite:\n",
    "- agrupar filas por una o varias columnas (por ejemplo, país)\n",
    "- calcular métricas resumidas (suma, media, máximo, mínimo, etc.)\n",
    "\n",
    "Esta es una de las operaciones más importantes en Spark porque suele:\n",
    "- requerir reorganizar los datos entre nodos del cluster\n",
    "- implicar un shuffle (movimiento de datos muy costoso)\n",
    "\n",
    "Si un alumno entiende que:\n",
    "\"agrupar implica juntar datos que pueden estar en máquinas distintas\"\n",
    "entonces tiene el modelo mental correcto de por qué ciertos pasos de Spark son caros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03f30e3f-dce2-4e23-a73f-9f2227636415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "# Aggregate total units and total sales by country\n",
    "sales_country_df = (\n",
    "    sales_with_total_df\n",
    "    .groupBy(\"country\")\n",
    "    .agg(\n",
    "        F.sum(\"units_sold\").alias(\"total_units\"),\n",
    "        F.sum(\"total_sales\").alias(\"total_sales_amount\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(sales_country_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78cfbb69-ae2b-4bfd-9cb1-08ccb99a2153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## orderBy(): ordenar datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02a2cf55-c6e7-4b8c-b4cc-6310d5e0c7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`orderBy()` sirve para ordenar las filas según una o varias columnas.\n",
    "\n",
    "Problema:\n",
    "- Ordenar \"globalmente\" en un sistema distribuido suele requerir:\n",
    "    - reunir datos\n",
    "    - redistribuirlos\n",
    "    - y coordinar el orden final\n",
    "\n",
    "Esto normalmente implica otro shuffle, y puede ser muy costoso en grandes volúmenes.\n",
    "\n",
    "Por eso, en pipelines reales:\n",
    "- hay que usar orderBy() solo cuando aporte valor\n",
    "- y evitar ordenar datasets gigantes si no es necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1980a306-2e8c-4019-9d25-0c2b66cd75ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "# Order countries by total sales amount (descending)\n",
    "sorted_sales_country_df = sales_country_df.orderBy(F.col(\"total_sales_amount\").desc())\n",
    "display(sorted_sales_country_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "350b35ea-4ec4-4e7f-95db-29d6ec9d6c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## join(): combinar datos de distintas tablas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c9270cb-5ea5-418c-b2f4-0609e0ea9d98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "En la práctica, casi siempre trabajamos con varias tablas:\n",
    "- una tabla de hechos (ventas, transacciones, etc.)\n",
    "- tablas de dimensiones (productos, clientes, países...)\n",
    "\n",
    "`join()` nos permite cruzar estas tablas:\n",
    "- por claves comunes (ej: `product`)\n",
    "\n",
    "Conceptos importantes:\n",
    "- Muchos joins implican shuffle, porque hay que alinear filas que pueden estar en nodos diferentes.\n",
    "- Spark ofrece optimizaciones como el broadcast join cuando una de las tablas es suficientemente pequeña para enviarla a todos los nodos.\n",
    "\n",
    "Entender bien los joins es clave para que Spark no \"se arrastre\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ce523b6-4652-44b5-9420-3f6518f8a4ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "# Read product dimension from volume\n",
    "product_dim_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/Volumes/sesion1/data/landing/product_dim.csv\")\n",
    ")\n",
    "\n",
    "display(product_dim_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53020c89-f2ce-482b-b53a-99cd865521ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "# Join sales with product dimension on 'product'\n",
    "sales_enriched_df = (\n",
    "    sales_with_total_df.alias(\"s\")\n",
    "    .join(\n",
    "        product_dim_df.alias(\"p\"),\n",
    "        on=\"product\",   # Join key\n",
    "        how=\"left\"     # Keep all sales even if some product has no dimension row\n",
    "    )\n",
    ")\n",
    "\n",
    "display(sales_enriched_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7daee26-e35f-4e38-9b88-b69813de4cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## write(): guardar el resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bd12e6c-58c4-4399-b1f7-5ae1527ba1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Un pipeline de datos útil no se queda solo en la pantalla:\n",
    "- debe guardar los resultados para que puedan ser reutilizados:\n",
    "    - por otros notebooks\n",
    "    - por herramientas de BI\n",
    "    - por servicios o modelos\n",
    "\n",
    "En Spark usamos la API write para:\n",
    "- escribir en distintos formatos (parquet, delta, csv, etc.)\n",
    "- salvar en tablas gestionadas por el catálogo\n",
    "\n",
    "Recordatorio:\n",
    "Sin write, muchas veces solo estamos \"jugando\" con los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "badb9204-c4fb-4b6a-aab6-1fd62205d27c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "# Save aggregated sales by country as a managed Delta table\n",
    "(\n",
    "    sorted_sales_country_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")          # Overwrite existing table if it exists\n",
    "    .saveAsTable(\"sesion1.sparkintro.sales_by_country\")\n",
    ")\n",
    "\n",
    "# Check that we can read it back as a table\n",
    "result_df = spark.table(\"sesion1.sparkintro.sales_by_country\")\n",
    "display(result_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d82e5a81-ee97-4034-a1ce-dc54a61a3851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "guia",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}